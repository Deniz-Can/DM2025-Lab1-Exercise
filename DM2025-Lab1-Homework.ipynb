{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Deniz Can Ozer 詹奧澤\n",
    "\n",
    "Student ID: NE6148418\n",
    "\n",
    "GitHub ID: https://github.com/Deniz-Can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Phase Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) that considered as **phase 1 (from exercise 1 to exercise 15)**. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** up **until phase 1**. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    -  Use [the new dataset](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/newdataset/Reddit-stock-sentiment.csv). The dataset contains a 16 columns including 'text' and 'label', with the sentiment labels being: 1.0 is positive, 0.0 is neutral and -1.0 is negative. You can simplify the dataset and use only the columns that you think are necessary. \n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "    - Use this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 10% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    \n",
    "\n",
    "\n",
    "4. Fourth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (September 28th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Phase Submission "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can keep the answer for phase 1 for easier running and update the phase 2 on the same page.**\n",
    "\n",
    "1. First: Continue doing the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) for **phase 2, starting from Finding frequent patterns**. Use the same master(.ipynb) file. Answer from phase 1 will not be considered at this stage. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: Continue from first phase and do the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** for phase 2, starting from Finding frequent pattern. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    - Continue using this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output. Use the same new dataset as in phase 1.\n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 20% of your grade.__\n",
    "    - Use this file to answer.\n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency).  Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences and when using augmentation with feature pattern.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 19th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#rows: 847, #columns: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>423</td>\n",
       "      <td>49.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>315</td>\n",
       "      <td>37.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>109</td>\n",
       "      <td>12.9%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count percentage\n",
       "label                  \n",
       " 0.0     423      49.9%\n",
       "-1.0     315      37.2%\n",
       " 1.0     109      12.9%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type             datetime  post_id       subreddit  \\\n",
      "0  comment  2025-04-11 17:29:56  mmli62w  wallstreetbets   \n",
      "1  comment   2025-04-12 1:12:19  mmnu7v9  wallstreetbets   \n",
      "2  comment  2025-04-10 15:09:41  mmeevio     StockMarket   \n",
      "\n",
      "                                               title                author  \\\n",
      "0    Retardation is on the menu boys! WSB is so back          StickyTip420   \n",
      "1  Retail giant TARGET has now declined for 10 co...  Comfortable-Dog-8437   \n",
      "2  How do you feel about a sitting president maki...          Btankersly66   \n",
      "\n",
      "                                                 url  upvotes  downvotes  \\\n",
      "0               https://i.redd.it/0yq2ftren8ue1.jpeg        0        NaN   \n",
      "1               https://i.redd.it/7tl6puv9waue1.jpeg      -15        NaN   \n",
      "2  https://apnews.com/article/trump-truth-social-...        1        NaN   \n",
      "\n",
      "   upvote_ratio                                               text  \\\n",
      "0           NaN                                   Calls on retards   \n",
      "1           NaN  Stunt as in like why did they even make a big ...   \n",
      "2           NaN                  Seeing lots of red in the ticker.   \n",
      "\n",
      "   subjectivity  polarity  sentiment                               entities  \\\n",
      "0      1.000000 -0.900000       -1.0                                     []   \n",
      "1      0.177778  0.083333        1.0  ['Stunt', 'company', 'deal', 'place']   \n",
      "2      0.000000  0.000000        0.0                             ['ticker']   \n",
      "\n",
      "   label  \n",
      "0   -1.0  \n",
      "1    0.0  \n",
      "2    0.0  \n",
      "   label label_name\n",
      "0   -1.0   negative\n",
      "1    0.0    neutral\n",
      "2    0.0    neutral\n",
      "3    1.0   positive\n",
      "4   -1.0   negative\n",
      "nr. of invalid or missing labels: 0\n",
      "#rows w/ missing/empty title: 0\n",
      "#rows w/ missing/empty text: 0\n"
     ]
    }
   ],
   "source": [
    "### Begin Assignment Here\n",
    "### same tasks as in phase 1 master notebook applied to the new dataset\n",
    "\n",
    "# necessary libraries\n",
    "#\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly as py\n",
    "import math \n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# general preparations & set-up\n",
    "#\n",
    "data_path = '/Users/denizcanozer/Documents/Uni_and_Career/TUM/TUM_MSc_RCI_2024/***TUM_MSc_RCI_Organizational/Exchange or Stay Abroad/TUMexchange at NCKU in WS2526/*NCKU Courses Fall 2025/1) DMCTA/DMCTA_Labs/Lab1/DM2025Labs/DM2025-Lab1-Exercise/newdataset/Reddit-stock-sentiment.csv'\n",
    "df_raw = pd.read_csv(data_path)\n",
    "n_rows_raw,n_columns_raw = df_raw.shape\n",
    "\n",
    "# how does the data table/ dimensionality look like:\n",
    "print(f\"#rows: {n_rows_raw}, #columns: {n_columns_raw}\")\n",
    "\n",
    "# nr. of records (rows) w/ a specific lable: to spot any imbalances\n",
    "counts = df_raw[\"label\"].value_counts(dropna=False)\n",
    "percentage_counts = (counts/counts.sum()*100).round(1)\n",
    "summary_counts = pd.DataFrame({\n",
    "    \"count\": counts,\n",
    "    \"percentage\": percentage_counts\n",
    "})\n",
    "summary_counts[\"percentage\"] = summary_counts[\"percentage\"].astype(str) + \"%\"\n",
    "display(summary_counts)\n",
    "print(df_raw.head(3))\n",
    "\n",
    "# translation of numeric lables to textual\n",
    "label_map ={\n",
    "    1.0: \"positive\", 0.0: \"neutral\", -1.0: \"negative\" \n",
    "}\n",
    "df_raw[\"label_name\"] = df_raw[\"label\"].map(label_map)\n",
    "print(df_raw[[\"label\",\"label_name\"]].head())           # prints first 5 labels\n",
    "\n",
    "\n",
    "# data preprocessing and preparation\n",
    "#\n",
    "# cols_to_drop = ['post_id','url','datetime','author','entities','type','downvotes','upvote_ratio']\n",
    "\n",
    "# find and drop all records w/ missing/ invalid target labels, keep all columns (actually 0)\n",
    "valid_labels = {-1, 0, 1}\n",
    "invalid_label_mask = ~df_raw[\"label\"].isin(valid_labels)\n",
    "n_invalid = invalid_label_mask.sum()\n",
    "print(f\"nr. of invalid or missing labels: {n_invalid}\")\n",
    "\n",
    "# find and drop all records w/ missing/ empty string text or titles (actually zero)\n",
    "invalid_title_mask = (\n",
    "    df_raw[\"title\"].isna() | df_raw[\"title\"].astype(str).str.strip().eq(\"\")\n",
    ")\n",
    "invalid_text_mask = (\n",
    "    df_raw[\"text\"].isna() | df_raw[\"text\"].astype(str).str.strip().eq(\"\")\n",
    ")\n",
    "print(f\"#rows w/ missing/empty title: {invalid_title_mask.sum()}\")\n",
    "print(f\"#rows w/ missing/empty text: {invalid_text_mask.sum()}\")\n",
    "\n",
    "# unused columns/ features to simplify the problem\n",
    "#most useful: title, text, label\n",
    "#useful for visualizations and meta-analysis: datetime, subreddit, type, upvotes, downvotes, upvote-ratio, entities\n",
    "#useful for debugging/ traceability: post-id\n",
    "#dropped columns: author, url, subjectivity, polarity, sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit entry title and text 1:\n",
      "Retardation is on the menu boys! WSB is so back | Calls on retards\n",
      "\n",
      "Label: -1.0 (label=negative)\n",
      "\n",
      "Reddit entry title and text 2:\n",
      "Retail giant TARGET has now declined for 10 consecutive weeks, its longest losing streak in history | Stunt as in like why did they even make a big deal about starting it in the first place? No company should ever talk about politics ever.\n",
      "\n",
      "Label: 0.0 (label=neutral)\n",
      "\n",
      "Reddit entry title and text 3:\n",
      "How do you feel about a sitting president making $415M in one day after pumping his own stock with social media and a policy decision? | Seeing lots of red in the ticker.\n",
      "\n",
      "Label: 0.0 (label=neutral)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### EXERCISE 1 ###\n",
    "# print out text data and category for the first 3 samples in the dataset\n",
    "# text data (title and text) and label will be printed out\n",
    "\n",
    "for i in range(3):\n",
    "    title_and_text = (df_raw.loc[i,\"title\"] + \" | \" + df_raw.loc[i,\"text\"]).strip()\n",
    "    print(f\"Reddit entry title and text {i+1}:\\n{title_and_text}\\n\")\n",
    "    print(f\"Label: {df_raw.loc[i,'label']} (label={df_raw.loc[i,'label_name']})\\n\")\n",
    "\n",
    "\n",
    "# # only text (not title)\n",
    "# for i in range(3):\n",
    "#     print(f\"Reddit entry text {i+1}:\\n{df_raw.loc[i,'text'].strip()}\\n\")\n",
    "#     print(f\"Label: {df_raw.loc[i,'label']} (label={df_raw.loc[i,'label_name']})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>datetime</th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-11 17:29:56</td>\n",
       "      <td>mmli62w</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Retardation is on the menu boys! WSB is so back</td>\n",
       "      <td>StickyTip420</td>\n",
       "      <td>https://i.redd.it/0yq2ftren8ue1.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calls on retards</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-12 1:12:19</td>\n",
       "      <td>mmnu7v9</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Retail giant TARGET has now declined for 10 co...</td>\n",
       "      <td>Comfortable-Dog-8437</td>\n",
       "      <td>https://i.redd.it/7tl6puv9waue1.jpeg</td>\n",
       "      <td>-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stunt as in like why did they even make a big ...</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Stunt', 'company', 'deal', 'place']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-10 15:09:41</td>\n",
       "      <td>mmeevio</td>\n",
       "      <td>StockMarket</td>\n",
       "      <td>How do you feel about a sitting president maki...</td>\n",
       "      <td>Btankersly66</td>\n",
       "      <td>https://apnews.com/article/trump-truth-social-...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seeing lots of red in the ticker.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['ticker']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>2023-08-30 17:12:55</td>\n",
       "      <td>165kllm</td>\n",
       "      <td>stockstobuytoday</td>\n",
       "      <td>Who knows more? $VMAR</td>\n",
       "      <td>emiljenfn</td>\n",
       "      <td>https://www.reddit.com/r/stockstobuytoday/comm...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>Vision Marine Technologies Inc. is rewriting t...</td>\n",
       "      <td>0.646970</td>\n",
       "      <td>0.216383</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['watercraft', 'skill', 'power', ']', 'feat', ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-11 14:48:05</td>\n",
       "      <td>mmkl6bw</td>\n",
       "      <td>StockMarket</td>\n",
       "      <td>The Trump administration is begging Xi Jinping...</td>\n",
       "      <td>Just-Big6411</td>\n",
       "      <td>https://edition.cnn.com/2025/04/10/politics/tr...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He didn’t say thank you.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type             datetime  post_id         subreddit  \\\n",
       "0  comment  2025-04-11 17:29:56  mmli62w    wallstreetbets   \n",
       "1  comment   2025-04-12 1:12:19  mmnu7v9    wallstreetbets   \n",
       "2  comment  2025-04-10 15:09:41  mmeevio       StockMarket   \n",
       "3     post  2023-08-30 17:12:55  165kllm  stockstobuytoday   \n",
       "4  comment  2025-04-11 14:48:05  mmkl6bw       StockMarket   \n",
       "\n",
       "                                               title                author  \\\n",
       "0    Retardation is on the menu boys! WSB is so back          StickyTip420   \n",
       "1  Retail giant TARGET has now declined for 10 co...  Comfortable-Dog-8437   \n",
       "2  How do you feel about a sitting president maki...          Btankersly66   \n",
       "3                              Who knows more? $VMAR             emiljenfn   \n",
       "4  The Trump administration is begging Xi Jinping...          Just-Big6411   \n",
       "\n",
       "                                                 url  upvotes  downvotes  \\\n",
       "0               https://i.redd.it/0yq2ftren8ue1.jpeg        0        NaN   \n",
       "1               https://i.redd.it/7tl6puv9waue1.jpeg      -15        NaN   \n",
       "2  https://apnews.com/article/trump-truth-social-...        1        NaN   \n",
       "3  https://www.reddit.com/r/stockstobuytoday/comm...       30        0.0   \n",
       "4  https://edition.cnn.com/2025/04/10/politics/tr...        1        NaN   \n",
       "\n",
       "   upvote_ratio                                               text  \\\n",
       "0           NaN                                   Calls on retards   \n",
       "1           NaN  Stunt as in like why did they even make a big ...   \n",
       "2           NaN                  Seeing lots of red in the ticker.   \n",
       "3          0.98  Vision Marine Technologies Inc. is rewriting t...   \n",
       "4           NaN                           He didn’t say thank you.   \n",
       "\n",
       "   subjectivity  polarity  sentiment  \\\n",
       "0      1.000000 -0.900000       -1.0   \n",
       "1      0.177778  0.083333        1.0   \n",
       "2      0.000000  0.000000        0.0   \n",
       "3      0.646970  0.216383        1.0   \n",
       "4      0.000000  0.000000        0.0   \n",
       "\n",
       "                                            entities  label label_name  \n",
       "0                                                 []   -1.0   negative  \n",
       "1              ['Stunt', 'company', 'deal', 'place']    0.0    neutral  \n",
       "2                                         ['ticker']    0.0    neutral  \n",
       "3  ['watercraft', 'skill', 'power', ']', 'feat', ...    1.0   positive  \n",
       "4                                                 []   -1.0   negative  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 315 entries labelled negative\n",
      "Found 3 entries containing the word 'TSLA'\n",
      "Found 79 entries in subreddit 'wallstreetbets' with neutral label (0)\n",
      "Found 424 entries which are either negative (-1) or positive (-1)\n",
      "Randomly selected 2 entries:\n",
      "        type             datetime  post_id       subreddit  \\\n",
      "457  comment  2025-04-11 15:42:58  mmkwda9     StockMarket   \n",
      "342  comment   2025-04-12 3:04:05  mmob98a  wallstreetbets   \n",
      "\n",
      "                                                 title              author  \\\n",
      "457  Trump: We Are Doing Really Well On Our Tariff ...  IamNotaMonkeyRobot   \n",
      "342  Retail giant TARGET has now declined for 10 co...    iPlayedHockeInHS   \n",
      "\n",
      "                                      url  upvotes  downvotes  upvote_ratio  \\\n",
      "457   https://i.redd.it/uajqu4kxl7ue1.png        1        NaN           NaN   \n",
      "342  https://i.redd.it/7tl6puv9waue1.jpeg        1        NaN           NaN   \n",
      "\n",
      "               text  subjectivity  polarity  sentiment               entities  \\\n",
      "457       \"We\" who?          0.00       0.0        0.0                     []   \n",
      "342  Chicken jockey          0.95      -0.6       -1.0  ['jockey', 'Chicken']   \n",
      "\n",
      "     label label_name  \n",
      "457    0.0    neutral  \n",
      "342    0.0    neutral  \n"
     ]
    }
   ],
   "source": [
    "### EXERCISE 2 ###\n",
    "# Experiment with other querying techniques using pandas dataframes. Refer to their documentation.\n",
    "\n",
    "# build Pandas data frame\n",
    "cols = [\n",
    "    \"type\",\"datetime\",\"post_id\",\"subreddit\",\"title\",\"author\",\"url\",\"upvotes\",\"downvotes\",\"upvote_ratio\",\"text\",\n",
    "    \"subjectivity\",\"polarity\",\"sentiment\",\"entities\",\"label\"\n",
    "]\n",
    "X = pd.DataFrame(df_raw[cols].copy())\n",
    "\n",
    "# add label_name to the frame\n",
    "X[\"label_name\"] = X[\"label\"].map(label_map)\n",
    "display(X.head())\n",
    "\n",
    "\n",
    "# query techniques\n",
    "\n",
    "# 1) boolean indexing (filtering by a condition such as specific label)\n",
    "# get all rows where label is -1\n",
    "negative_entries = X[X[\"label\"] == -1]\n",
    "print(f\"Found {len(negative_entries)} entries labelled negative\")\n",
    "\n",
    "# 2) using string search (filtering by text content)\n",
    "# get all rows where text contains the word 'TSLA' (case-insensitive)\n",
    "tsla_entries = X[X[\"text\"].str.contains(\"TSLA\",case=False,na=False)]\n",
    "print(f\"Found {len(tsla_entries)} entries containing the word 'TSLA'\")\n",
    "\n",
    "# 3) the .query() method (filtering by complex/ specific conditions like SQL)\n",
    "# get all rows where subreddit is \"wallstreetbets\" and label is neutral\n",
    "wsb_neutral_entries = X.query('subreddit == \"wallstreetbets\" and label == 0')\n",
    "print(f\"Found {len(wsb_neutral_entries)} entries in subreddit 'wallstreetbets' with neutral label (0)\")\n",
    "\n",
    "# 4) the .isin() method (filtering by a list of values)\n",
    "# useful for filtering by multiple labels without using long OR statements\n",
    "target_labels = [-1,1]\n",
    "polarizing_entries = X[X[\"label\"].isin(target_labels)]\n",
    "print(f\"Found {len(polarizing_entries)} entries which are either negative (-1) or positive (-1)\")\n",
    "\n",
    "# 5) random sampling with .sample() (getting a random subset of entries)\n",
    "random_entries = X.sample(n=2, random_state=42)\n",
    "print(\"Randomly selected 2 entries:\")\n",
    "print(random_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>datetime</th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-12 1:12:19</td>\n",
       "      <td>mmnu7v9</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Retail giant TARGET has now declined for 10 co...</td>\n",
       "      <td>Comfortable-Dog-8437</td>\n",
       "      <td>https://i.redd.it/7tl6puv9waue1.jpeg</td>\n",
       "      <td>-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stunt as in like why did they even make a big ...</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Stunt', 'company', 'deal', 'place']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-12 2:46:28</td>\n",
       "      <td>mmo8obq</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Retail giant TARGET has now declined for 10 co...</td>\n",
       "      <td>tiredsultan</td>\n",
       "      <td>https://i.redd.it/7tl6puv9waue1.jpeg</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google \"target dei\".\\n\\nI did it for you.\\n\\n</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['dei', 'Google', 'target']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-11 22:00:36</td>\n",
       "      <td>mmmyq7m</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Weekend Discussion Thread for the Weekend of A...</td>\n",
       "      <td>InstructionNo4546</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>These China AI factory memes are pretty good. ...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['factory', 'time', 'AI', 'China AI', 'China',...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-11 16:00:46</td>\n",
       "      <td>mml01xt</td>\n",
       "      <td>stocks</td>\n",
       "      <td>The US bond market is continuing to crash. Wil...</td>\n",
       "      <td>J3ster14</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1jwsz...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The steal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['steal']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>post</td>\n",
       "      <td>2025-04-04 6:25:04</td>\n",
       "      <td>1jr5ag5</td>\n",
       "      <td>stockstobuytoday</td>\n",
       "      <td>Spending first $250 tmmw on stocks</td>\n",
       "      <td>Alarmed_Champion_913</td>\n",
       "      <td>https://www.reddit.com/r/stockstobuytoday/comm...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A lot of stocks are down right now, and I'm th...</td>\n",
       "      <td>0.347959</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['cuz', 'OPTT', 'BLGO', 'im', 'money', 'idea',...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type             datetime  post_id         subreddit  \\\n",
       "1   comment   2025-04-12 1:12:19  mmnu7v9    wallstreetbets   \n",
       "22  comment   2025-04-12 2:46:28  mmo8obq    wallstreetbets   \n",
       "40  comment  2025-04-11 22:00:36  mmmyq7m    wallstreetbets   \n",
       "55  comment  2025-04-11 16:00:46  mml01xt            stocks   \n",
       "72     post   2025-04-04 6:25:04  1jr5ag5  stockstobuytoday   \n",
       "\n",
       "                                                title                author  \\\n",
       "1   Retail giant TARGET has now declined for 10 co...  Comfortable-Dog-8437   \n",
       "22  Retail giant TARGET has now declined for 10 co...           tiredsultan   \n",
       "40  Weekend Discussion Thread for the Weekend of A...     InstructionNo4546   \n",
       "55  The US bond market is continuing to crash. Wil...              J3ster14   \n",
       "72                 Spending first $250 tmmw on stocks  Alarmed_Champion_913   \n",
       "\n",
       "                                                  url  upvotes  downvotes  \\\n",
       "1                https://i.redd.it/7tl6puv9waue1.jpeg      -15        NaN   \n",
       "22               https://i.redd.it/7tl6puv9waue1.jpeg        5        NaN   \n",
       "40  https://www.reddit.com/r/wallstreetbets/commen...       10        NaN   \n",
       "55  https://www.reddit.com/r/stocks/comments/1jwsz...        1        NaN   \n",
       "72  https://www.reddit.com/r/stockstobuytoday/comm...        1        0.0   \n",
       "\n",
       "    upvote_ratio                                               text  \\\n",
       "1            NaN  Stunt as in like why did they even make a big ...   \n",
       "22           NaN      Google \"target dei\".\\n\\nI did it for you.\\n\\n   \n",
       "40           NaN  These China AI factory memes are pretty good. ...   \n",
       "55           NaN                                          The steal   \n",
       "72           1.0  A lot of stocks are down right now, and I'm th...   \n",
       "\n",
       "    subjectivity  polarity  sentiment  \\\n",
       "1       0.177778  0.083333        1.0   \n",
       "22      0.000000  0.000000        0.0   \n",
       "40      0.800000  0.475000        1.0   \n",
       "55      0.000000  0.000000        0.0   \n",
       "72      0.347959  0.005102        1.0   \n",
       "\n",
       "                                             entities  label label_name  \n",
       "1               ['Stunt', 'company', 'deal', 'place']    0.0    neutral  \n",
       "22                        ['dei', 'Google', 'target']    0.0    neutral  \n",
       "40  ['factory', 'time', 'AI', 'China AI', 'China',...    0.0    neutral  \n",
       "55                                          ['steal']    0.0    neutral  \n",
       "72  ['cuz', 'OPTT', 'BLGO', 'im', 'money', 'idea',...    0.0    neutral  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### EXERCISE 3 ###\n",
    "\n",
    "# filter by 'neutral' label and query every 10th record and print only the first 5 results\n",
    "\n",
    "X[X[\"label\"] == 0][::10].head(5)\n",
    "\n",
    "# or just display selected columns\n",
    "# X[X[\"label\"] == 0][::10].head(5)[[\"title\",\"text\",\"label\",\"label_name\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      2\n",
       "2      2\n",
       "3      0\n",
       "4      2\n",
       "      ..\n",
       "842    2\n",
       "843    2\n",
       "844    0\n",
       "845    2\n",
       "846    2\n",
       "Length: 847, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### EXERCISE 4 ###\n",
    "# calculate missing values per row (for every record) using axis\n",
    "# returns a panda series with the 1st column showing the document index and the 2nd column showing the count of missing values in that row\n",
    "\n",
    "X.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE 5 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab1-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
